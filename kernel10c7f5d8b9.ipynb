{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee73c77c7478ef18dc13e4e3fb087657296d4a3c","scrolled":true},"cell_type":"code","source":"# Import data\ndata = pd.read_csv('../input/data_stocks.csv')\n\n# Drop date variable\ndata = data.drop(['DATE'], 1)\n\nprint(data.head())\n\n# Dimensions of dataset\nn = data.shape[0]\np = data.shape[1]\n\n# Make data a np.array\ndata = data.values\n\n# Training and test data\ntrain_start = 0\ntrain_end = int(np.floor(0.8*n))\ntest_start = train_end + 1\ntest_end = n\ndata_train = data[np.arange(train_start, train_end), :]\ndata_test = data[np.arange(test_start, test_end), :]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c78433e6d85e788fe04c9597036dde25b0e4e36e"},"cell_type":"code","source":"# Scale data\nscaler = MinMaxScaler(feature_range=(-1, 1))\nscaler.fit(data_train)\ndata_train = scaler.transform(data_train)\ndata_test = scaler.transform(data_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1b9b414c0833d0c05a2a89cb62184d0a511cd81"},"cell_type":"code","source":"# Build X and y\nX_train = data_train[:, 1:]\ny_train = data_train[:, 0]\nX_test = data_test[:, 1:]\ny_test = data_test[:, 0]\n\n# Number of stocks in training data\nn_stocks = X_train.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec03fc61f813d92129232d2463524fed7568162a"},"cell_type":"code","source":"# Neurons\nn_neurons_1 = 1024\nn_neurons_2 = 512\nn_neurons_3 = 256\nn_neurons_4 = 128\n\n# Session\nnet = tf.InteractiveSession()\n\n# Placeholder\nX = tf.placeholder(dtype=tf.float32, shape=[None, n_stocks])\nY = tf.placeholder(dtype=tf.float32, shape=[None])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc8975f2042843883cd55eacf3011b586f5b4e4b"},"cell_type":"code","source":"# Initializers\nsigma = 1\nweight_initializer = tf.variance_scaling_initializer(mode=\"fan_avg\", distribution=\"uniform\", scale=sigma)\nbias_initializer = tf.zeros_initializer()\n\n# Hidden weights\nW_hidden_1 = tf.Variable(weight_initializer([n_stocks, n_neurons_1]))\nbias_hidden_1 = tf.Variable(bias_initializer([n_neurons_1]))\nW_hidden_2 = tf.Variable(weight_initializer([n_neurons_1, n_neurons_2]))\nbias_hidden_2 = tf.Variable(bias_initializer([n_neurons_2]))\nW_hidden_3 = tf.Variable(weight_initializer([n_neurons_2, n_neurons_3]))\nbias_hidden_3 = tf.Variable(bias_initializer([n_neurons_3]))\nW_hidden_4 = tf.Variable(weight_initializer([n_neurons_3, n_neurons_4]))\nbias_hidden_4 = tf.Variable(bias_initializer([n_neurons_4]))\n\n# Output weights\nW_out = tf.Variable(weight_initializer([n_neurons_4, 1]))\nbias_out = tf.Variable(bias_initializer([1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d31d0cf7dd8dbbc5c22fe9e266dc1b6e42d790d9"},"cell_type":"code","source":"# Hidden layer\nhidden_1 = tf.nn.relu(tf.add(tf.matmul(X, W_hidden_1), bias_hidden_1))\nhidden_2 = tf.nn.relu(tf.add(tf.matmul(hidden_1, W_hidden_2), bias_hidden_2))\nhidden_3 = tf.nn.relu(tf.add(tf.matmul(hidden_2, W_hidden_3), bias_hidden_3))\nhidden_4 = tf.nn.relu(tf.add(tf.matmul(hidden_3, W_hidden_4), bias_hidden_4))\n\n# Output layer (transpose!)\nout = tf.transpose(tf.add(tf.matmul(hidden_4, W_out), bias_out))\n\n# Cost function\nmse = tf.reduce_mean(tf.squared_difference(out, Y))\n\n# Optimizer\nopt = tf.train.AdamOptimizer().minimize(mse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c18ca926ae365eeb258a3dfc3c46ee948bba0249"},"cell_type":"code","source":"# Init\nnet.run(tf.global_variables_initializer())\n\n# Setup plot\nplt.ion()\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nline1, = ax1.plot(y_test)\nline2, = ax1.plot(y_test * 0.5)\nplt.show()\n\n# Fit neural net\nbatch_size = 256\nmse_train = []\nmse_test = []\n\n# Run\nepochs = 10\nfor e in range(epochs):\n\n    # Shuffle training data\n    shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n    X_train = X_train[shuffle_indices]\n    y_train = y_train[shuffle_indices]\n\n    # Minibatch training\n    for i in range(0, len(y_train) // batch_size):\n        start = i * batch_size\n        batch_x = X_train[start:start + batch_size]\n        batch_y = y_train[start:start + batch_size]\n        # Run optimizer with batch\n        net.run(opt, feed_dict={X: batch_x, Y: batch_y})\n\n        # Show progress\n        if np.mod(i, 50) == 0:\n            # MSE train and test\n            mse_train.append(net.run(mse, feed_dict={X: X_train, Y: y_train}))\n            mse_test.append(net.run(mse, feed_dict={X: X_test, Y: y_test}))\n            print('MSE Train: ', mse_train[-1])\n            print('MSE Test: ', mse_test[-1])\n            # Prediction\n            pred = net.run(out, feed_dict={X: X_test})\n            line2.set_ydata(pred)\n            plt.title('Epoch ' + str(e) + ', Batch ' + str(i))\nplt.pause(0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"058e00fa330c8f53e4c01184031ee7f990fbb6c5"},"cell_type":"code","source":"#! /usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\nimport argparse\nimport tensorflow as tf\nimport numpy as np\n\n\ndef open_interactive_session():\n    A = tf.Variable(np.random.randn(16, 255, 255, 3).astype(np.float32))\n    sess = tf.InteractiveSession()\n    sess.run(tf.global_variables_initializer())\n\n\ndef open_and_close_interactive_session():\n    A = tf.Variable(np.random.randn(16, 255, 255, 3).astype(np.float32))\n    sess = tf.InteractiveSession()\n    sess.run(tf.global_variables_initializer())\n    sess.close()\n\n\ndef open_and_close_session():\n    A = tf.Variable(np.random.randn(16, 255, 255, 3).astype(np.float32))\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--num', help='repeat', type=int, default=5)\n    parser.add_argument('type', choices=['interactive', 'interactive_close', 'normal'])\n    args = parser.parse_args()\n\n    sess_func = open_and_close_session\n\n    if args.type == 'interactive':\n        sess_func = open_interactive_session\n    elif args.type == 'interactive_close':\n        sess_func = open_and_close_interactive_session\n\n    for _ in range(args.num):\n        sess_func()\n    with tf.Session() as sess:\n        print(\"bytes used=\", sess.run(tf.contrib.memory_stats.BytesInUse()))\ngives","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}